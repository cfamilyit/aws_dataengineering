{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a028ca46",
   "metadata": {},
   "source": [
    "# <center> PySpark Programming Introduction</center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bcb3f92e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Welcome to PySpark World..!\n",
      "******************************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n",
      "******************************\n",
      "spark hadoop spark hive\n",
      "spark spark hive hive\n",
      "hadoop hdfs hadoop hdfs\n",
      "hadoop hdfs hive spark\n",
      "******************************\n",
      "spark hadoop spark hive\n",
      "spark spark hive hive\n",
      "******************************\n"
     ]
    }
   ],
   "source": [
    "print(\"Welcome to PySpark World..!\")\n",
    "print(\"*\"*30)\n",
    "rdd1 = sc.textFile(\"/home/ubuntu/data/comment.txt\")\n",
    "print(rdd1.count())\n",
    "print(\"*\"*30)\n",
    "for row in rdd1.collect():\n",
    "    print(row)\n",
    "print(\"*\"*30)\n",
    "for row in rdd1.take(2):\n",
    "    print(row)\n",
    "print(\"*\"*30)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfab573d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9709e216",
   "metadata": {},
   "source": [
    "## <center> PySpark Core Programming </center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a0a0e3a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<SparkContext master=local[*] appName=PySparkShell>\n",
      "<class 'pyspark.context.SparkContext'>\n"
     ]
    }
   ],
   "source": [
    "print(sc)\n",
    "print(type(sc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72eb339e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9b9d89e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<pyspark.sql.session.SparkSession object at 0x7fb6381dbe20>\n",
      "<class 'pyspark.sql.session.SparkSession'>\n"
     ]
    }
   ],
   "source": [
    "print(spark)\n",
    "print(type(spark))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e54280a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1eca8d4b",
   "metadata": {},
   "source": [
    "### Write a PySpark Program to find count, display all rows and display few rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c8612255",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n",
      "******************************\n",
      "spark hadoop spark hive\n",
      "spark spark hive hive\n",
      "hadoop hdfs hadoop hdfs\n",
      "hadoop hdfs hive spark\n",
      "******************************\n",
      "spark hadoop spark hive\n",
      "spark spark hive hive\n",
      "******************************\n"
     ]
    }
   ],
   "source": [
    "#creating RDD using SparkSession\n",
    "rdd1 = spark.sparkContext.textFile(\"/home/ubuntu/data/comment.txt\")\n",
    "\n",
    "#Count No.of rows an RDD\n",
    "print(rdd1.count())\n",
    "print(\"*\"*30)\n",
    "\n",
    "#Display all rows an RDD\n",
    "for row in rdd1.collect():\n",
    "    print(row)\n",
    "print(\"*\"*30)\n",
    "\n",
    "#Display few rows an RDD\n",
    "for row in rdd1.take(2):\n",
    "    print(row)\n",
    "print(\"*\"*30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "863a02a6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "67ba2f89",
   "metadata": {},
   "source": [
    "### Write PySpark Program to find WordCount (Word Repeated How many time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "929ab63d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All rows an RDD:\n",
      "******************************\n",
      "spark hadoop spark hive\n",
      "spark spark hive hive\n",
      "hadoop hdfs hadoop hdfs\n",
      "hadoop hdfs hive spark\n",
      "******************************\n",
      "After flatMap transformation, All rows an RDD:\n",
      "******************************\n",
      "spark\n",
      "hadoop\n",
      "spark\n",
      "hive\n",
      "spark\n",
      "spark\n",
      "hive\n",
      "hive\n",
      "hadoop\n",
      "hdfs\n",
      "hadoop\n",
      "hdfs\n",
      "hadoop\n",
      "hdfs\n",
      "hive\n",
      "spark\n",
      "******************************\n",
      "After map transformation, All rows an RDD:\n",
      "******************************\n",
      "('spark', 1)\n",
      "('hadoop', 1)\n",
      "('spark', 1)\n",
      "('hive', 1)\n",
      "('spark', 1)\n",
      "('spark', 1)\n",
      "('hive', 1)\n",
      "('hive', 1)\n",
      "('hadoop', 1)\n",
      "('hdfs', 1)\n",
      "('hadoop', 1)\n",
      "('hdfs', 1)\n",
      "('hadoop', 1)\n",
      "('hdfs', 1)\n",
      "('hive', 1)\n",
      "('spark', 1)\n",
      "******************************\n",
      "After reduceBy transformation, All rows an RDD:\n",
      "******************************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 16:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('spark', 5)\n",
      "('hadoop', 4)\n",
      "('hive', 4)\n",
      "('hdfs', 3)\n",
      "******************************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "#creating RDD using SparkSession\n",
    "rdd1 = spark.sparkContext.textFile(\"/home/ubuntu/data/comment.txt\")\n",
    "\n",
    "print(\"All rows an RDD:\")\n",
    "print(\"*\"*30)\n",
    "for row in rdd1.collect():\n",
    "    print(row)\n",
    "print(\"*\"*30)\n",
    "\n",
    "rdd2 = rdd1.flatMap(lambda x:x.split(\" \"))\n",
    "print(\"After flatMap transformation, All rows an RDD:\")\n",
    "print(\"*\"*30)\n",
    "for row in rdd2.collect():\n",
    "    print(row)\n",
    "print(\"*\"*30)\n",
    "\n",
    "rdd3 = rdd2.map(lambda x:(x,1))\n",
    "print(\"After map transformation, All rows an RDD:\")\n",
    "print(\"*\"*30)\n",
    "for row in rdd3.collect():\n",
    "    print(row)\n",
    "print(\"*\"*30)\n",
    "\n",
    "rdd4 = rdd3.reduceByKey(lambda x,y:(x+y))\n",
    "print(\"After reduceBy transformation, All rows an RDD:\")\n",
    "print(\"*\"*30)\n",
    "for row in rdd4.collect():\n",
    "    print(row)\n",
    "print(\"*\"*30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34834661",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef843d75",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25b543ac",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15dc2172",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9bb9f8d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "802fb45d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87aad333",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a19bad8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ede4b474",
   "metadata": {},
   "source": [
    "# <center> PySpark SQL </center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74e7ef66",
   "metadata": {},
   "source": [
    "### Write a PySpark SQL to create DataFrame from CSV and Apply DSL Queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7f2becd9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- _c0: string (nullable = true)\n",
      " |-- _c1: string (nullable = true)\n",
      " |-- _c2: string (nullable = true)\n",
      " |-- _c3: string (nullable = true)\n",
      " |-- _c4: string (nullable = true)\n",
      " |-- _c5: string (nullable = true)\n",
      " |-- _c6: string (nullable = true)\n",
      " |-- _c7: string (nullable = true)\n",
      " |-- _c8: string (nullable = true)\n",
      " |-- _c9: string (nullable = true)\n",
      " |-- _c10: string (nullable = true)\n",
      " |-- _c11: string (nullable = true)\n",
      "\n",
      "+------+-----+---+---+--------+-----------------+---------+---------+--------+-----+---------+--------+\n",
      "|   _c0|  _c1|_c2|_c3|     _c4|              _c5|      _c6|      _c7|     _c8|  _c9|     _c10|    _c11|\n",
      "+------+-----+---+---+--------+-----------------+---------+---------+--------+-----+---------+--------+\n",
      "|    ID| Name|Age|Sex|   State|         Symptoms|Diagnosis|   Cancer|CancerSc|Stage|Treatment|Survival|\n",
      "|100001|David| 45|  M| ALAMAMA|RED ITCHY PATCHES|   BIOPSY|MALIGNANT|    SKIN|    1|RESECTION|     YES|\n",
      "|100002| John| 56|  M|  ALASKA|      BLOOD COUGH| PET SCAN|MALIGNANT|THORACIC|    2|  SURGERY|     YES|\n",
      "|100003| Paul| 65|  M| ARIZONA|RED ITCHY PATCHES|   BIOPSY|MALIGNANT|    SKIN|    3|RESECTION|      NO|\n",
      "|100004| Mark| 35|  M|ARKANSAS|      BLOOD COUGH| PET SCAN|MALIGNANT|THORACIC|    3|  SURGERY|      NO|\n",
      "+------+-----+---+---+--------+-----------------+---------+---------+--------+-----+---------+--------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cancerdataDF = spark.read.csv(\"/home/ubuntu/data/CancerData.csv\")\n",
    "cancerdataDF.printSchema()\n",
    "cancerdataDF.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ad96c05",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4deb7bb7",
   "metadata": {},
   "source": [
    "### Write a PySpark SQL to create DataFrame from CSV by intimating header and Apply DSL Queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "eec43f95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- ID: string (nullable = true)\n",
      " |-- Name: string (nullable = true)\n",
      " |-- Age: string (nullable = true)\n",
      " |-- Sex: string (nullable = true)\n",
      " |-- State: string (nullable = true)\n",
      " |-- Symptoms: string (nullable = true)\n",
      " |-- Diagnosis: string (nullable = true)\n",
      " |-- Cancer: string (nullable = true)\n",
      " |-- CancerSc: string (nullable = true)\n",
      " |-- Stage: string (nullable = true)\n",
      " |-- Treatment: string (nullable = true)\n",
      " |-- Survival: string (nullable = true)\n",
      "\n",
      "+------+-----+---+---+----------+-----------------+---------+---------+--------+-----+---------+--------+\n",
      "|    ID| Name|Age|Sex|     State|         Symptoms|Diagnosis|   Cancer|CancerSc|Stage|Treatment|Survival|\n",
      "+------+-----+---+---+----------+-----------------+---------+---------+--------+-----+---------+--------+\n",
      "|100001|David| 45|  M|   ALAMAMA|RED ITCHY PATCHES|   BIOPSY|MALIGNANT|    SKIN|    1|RESECTION|     YES|\n",
      "|100002| John| 56|  M|    ALASKA|      BLOOD COUGH| PET SCAN|MALIGNANT|THORACIC|    2|  SURGERY|     YES|\n",
      "|100003| Paul| 65|  M|   ARIZONA|RED ITCHY PATCHES|   BIOPSY|MALIGNANT|    SKIN|    3|RESECTION|      NO|\n",
      "|100004| Mark| 35|  M|  ARKANSAS|      BLOOD COUGH| PET SCAN|MALIGNANT|THORACIC|    3|  SURGERY|      NO|\n",
      "|100005|James| 44|  M|CALIFORNIA|RED ITCHY PATCHES|   BIOPSY|MALIGNANT|    SKIN|    2|RESECTION|     YES|\n",
      "+------+-----+---+---+----------+-----------------+---------+---------+--------+-----+---------+--------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cancerdataDF = spark.read.csv(\"/home/ubuntu/data/CancerData.csv\",header=True)\n",
    "cancerdataDF.printSchema()\n",
    "cancerdataDF.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5a5eb00",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7c221c08",
   "metadata": {},
   "source": [
    "### Write a PySpark SQL to create DataFrame from CSV by intimating header and Inferring Schemaand Apply DSL Queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3af8553f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- ID: integer (nullable = true)\n",
      " |-- Name: string (nullable = true)\n",
      " |-- Age: integer (nullable = true)\n",
      " |-- Sex: string (nullable = true)\n",
      " |-- State: string (nullable = true)\n",
      " |-- Symptoms: string (nullable = true)\n",
      " |-- Diagnosis: string (nullable = true)\n",
      " |-- Cancer: string (nullable = true)\n",
      " |-- CancerSc: string (nullable = true)\n",
      " |-- Stage: string (nullable = true)\n",
      " |-- Treatment: string (nullable = true)\n",
      " |-- Survival: string (nullable = true)\n",
      "\n",
      "+------+-----+---+---+----------+-----------------+---------+---------+--------+-----+---------+--------+\n",
      "|    ID| Name|Age|Sex|     State|         Symptoms|Diagnosis|   Cancer|CancerSc|Stage|Treatment|Survival|\n",
      "+------+-----+---+---+----------+-----------------+---------+---------+--------+-----+---------+--------+\n",
      "|100001|David| 45|  M|   ALAMAMA|RED ITCHY PATCHES|   BIOPSY|MALIGNANT|    SKIN|    1|RESECTION|     YES|\n",
      "|100002| John| 56|  M|    ALASKA|      BLOOD COUGH| PET SCAN|MALIGNANT|THORACIC|    2|  SURGERY|     YES|\n",
      "|100003| Paul| 65|  M|   ARIZONA|RED ITCHY PATCHES|   BIOPSY|MALIGNANT|    SKIN|    3|RESECTION|      NO|\n",
      "|100004| Mark| 35|  M|  ARKANSAS|      BLOOD COUGH| PET SCAN|MALIGNANT|THORACIC|    3|  SURGERY|      NO|\n",
      "|100005|James| 44|  M|CALIFORNIA|RED ITCHY PATCHES|   BIOPSY|MALIGNANT|    SKIN|    2|RESECTION|     YES|\n",
      "+------+-----+---+---+----------+-----------------+---------+---------+--------+-----+---------+--------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cancerdataDF = spark.read.csv(\"/home/ubuntu/data/CancerData.csv\",header=True,inferSchema=True)\n",
    "cancerdataDF.printSchema()\n",
    "cancerdataDF.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd4d0120",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "fd0aa772",
   "metadata": {},
   "source": [
    "### How Write DSL Queries on DataFrames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ba96d43e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All rows and all columns:\n",
      "******************************\n",
      "+------+-----+---+---+----------+-----------------+---------+---------+--------+-----+---------+--------+\n",
      "|    ID| Name|Age|Sex|     State|         Symptoms|Diagnosis|   Cancer|CancerSc|Stage|Treatment|Survival|\n",
      "+------+-----+---+---+----------+-----------------+---------+---------+--------+-----+---------+--------+\n",
      "|100001|David| 45|  M|   ALAMAMA|RED ITCHY PATCHES|   BIOPSY|MALIGNANT|    SKIN|    1|RESECTION|     YES|\n",
      "|100002| John| 56|  M|    ALASKA|      BLOOD COUGH| PET SCAN|MALIGNANT|THORACIC|    2|  SURGERY|     YES|\n",
      "|100003| Paul| 65|  M|   ARIZONA|RED ITCHY PATCHES|   BIOPSY|MALIGNANT|    SKIN|    3|RESECTION|      NO|\n",
      "|100004| Mark| 35|  M|  ARKANSAS|      BLOOD COUGH| PET SCAN|MALIGNANT|THORACIC|    3|  SURGERY|      NO|\n",
      "|100005|James| 44|  M|CALIFORNIA|RED ITCHY PATCHES|   BIOPSY|MALIGNANT|    SKIN|    2|RESECTION|     YES|\n",
      "+------+-----+---+---+----------+-----------------+---------+---------+--------+-----+---------+--------+\n",
      "only showing top 5 rows\n",
      "\n",
      "******************************\n",
      "All rows but few columns:\n",
      "******************************\n",
      "+------+-----+---+---+\n",
      "|    ID| Name|Age|Sex|\n",
      "+------+-----+---+---+\n",
      "|100001|David| 45|  M|\n",
      "|100002| John| 56|  M|\n",
      "|100003| Paul| 65|  M|\n",
      "|100004| Mark| 35|  M|\n",
      "|100005|James| 44|  M|\n",
      "+------+-----+---+---+\n",
      "only showing top 5 rows\n",
      "\n",
      "******************************\n",
      "All rows but only gender=male and age>50:\n",
      "******************************\n",
      "+------+------+---+---+-----------+-----------------+---------+---------+--------+-----+---------+--------+\n",
      "|    ID|  Name|Age|Sex|      State|         Symptoms|Diagnosis|   Cancer|CancerSc|Stage|Treatment|Survival|\n",
      "+------+------+---+---+-----------+-----------------+---------+---------+--------+-----+---------+--------+\n",
      "|100002|  John| 56|  M|     ALASKA|      BLOOD COUGH| PET SCAN|MALIGNANT|THORACIC|    2|  SURGERY|     YES|\n",
      "|100003|  Paul| 65|  M|    ARIZONA|RED ITCHY PATCHES|   BIOPSY|MALIGNANT|    SKIN|    3|RESECTION|      NO|\n",
      "|100006|Andrew| 53|  M|   COLORADO|RED ITCHY PATCHES|   BIOPSY|MALIGNANT|    SKIN|    1|RESECTION|     YES|\n",
      "|100007| Scott| 68|  M|CONNECTICUT|      BLOOD COUGH| PET SCAN|MALIGNANT|THORACIC|    1|  SURGERY|     YES|\n",
      "|100009|RoMert| 54|  M|    FLORIDA|      BLOOD COUGH| PET SCAN|MALIGNANT|THORACIC|    1|  SURGERY|     YES|\n",
      "+------+------+---+---+-----------+-----------------+---------+---------+--------+-----+---------+--------+\n",
      "only showing top 5 rows\n",
      "\n",
      "******************************\n"
     ]
    }
   ],
   "source": [
    "cancerdataDF = spark.read.csv(\"/home/ubuntu/data/CancerData.csv\",header=True,inferSchema=True)\n",
    "print(\"All rows and all columns:\")\n",
    "print(\"*\"*30)\n",
    "cancerdataDF.select(\"*\").show(5)\n",
    "print(\"*\"*30)\n",
    "\n",
    "print(\"All rows but few columns:\")\n",
    "print(\"*\"*30)\n",
    "cancerdataDF.select(\"ID\",\"Name\",\"Age\",\"Sex\").show(5)\n",
    "print(\"*\"*30)\n",
    "\n",
    "print(\"All rows but only gender=male and age>50:\")\n",
    "print(\"*\"*30)\n",
    "cancerdataDF.select(\"*\").where(\"Sex='M'\").where(\"Age>50\").show(5)\n",
    "print(\"*\"*30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b20d522",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0535579f",
   "metadata": {},
   "source": [
    "### How Write Native SQL Queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fc205532",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All rows and all columns:\n",
      "******************************\n",
      "+------+-----+---+---+----------+-----------------+---------+---------+--------+-----+---------+--------+\n",
      "|    ID| Name|Age|Sex|     State|         Symptoms|Diagnosis|   Cancer|CancerSc|Stage|Treatment|Survival|\n",
      "+------+-----+---+---+----------+-----------------+---------+---------+--------+-----+---------+--------+\n",
      "|100001|David| 45|  M|   ALAMAMA|RED ITCHY PATCHES|   BIOPSY|MALIGNANT|    SKIN|    1|RESECTION|     YES|\n",
      "|100002| John| 56|  M|    ALASKA|      BLOOD COUGH| PET SCAN|MALIGNANT|THORACIC|    2|  SURGERY|     YES|\n",
      "|100003| Paul| 65|  M|   ARIZONA|RED ITCHY PATCHES|   BIOPSY|MALIGNANT|    SKIN|    3|RESECTION|      NO|\n",
      "|100004| Mark| 35|  M|  ARKANSAS|      BLOOD COUGH| PET SCAN|MALIGNANT|THORACIC|    3|  SURGERY|      NO|\n",
      "|100005|James| 44|  M|CALIFORNIA|RED ITCHY PATCHES|   BIOPSY|MALIGNANT|    SKIN|    2|RESECTION|     YES|\n",
      "+------+-----+---+---+----------+-----------------+---------+---------+--------+-----+---------+--------+\n",
      "only showing top 5 rows\n",
      "\n",
      "All rows but few columns:\n",
      "******************************\n",
      "+------+-----+---+---+\n",
      "|    ID| Name|Age|Sex|\n",
      "+------+-----+---+---+\n",
      "|100001|David| 45|  M|\n",
      "|100002| John| 56|  M|\n",
      "|100003| Paul| 65|  M|\n",
      "|100004| Mark| 35|  M|\n",
      "|100005|James| 44|  M|\n",
      "+------+-----+---+---+\n",
      "only showing top 5 rows\n",
      "\n",
      "All rows but only gender=male and age>50:\n",
      "******************************\n",
      "+------+------+---+---+-----------+-----------------+---------+---------+--------+-----+---------+--------+\n",
      "|    ID|  Name|Age|Sex|      State|         Symptoms|Diagnosis|   Cancer|CancerSc|Stage|Treatment|Survival|\n",
      "+------+------+---+---+-----------+-----------------+---------+---------+--------+-----+---------+--------+\n",
      "|100002|  John| 56|  M|     ALASKA|      BLOOD COUGH| PET SCAN|MALIGNANT|THORACIC|    2|  SURGERY|     YES|\n",
      "|100003|  Paul| 65|  M|    ARIZONA|RED ITCHY PATCHES|   BIOPSY|MALIGNANT|    SKIN|    3|RESECTION|      NO|\n",
      "|100006|Andrew| 53|  M|   COLORADO|RED ITCHY PATCHES|   BIOPSY|MALIGNANT|    SKIN|    1|RESECTION|     YES|\n",
      "|100007| Scott| 68|  M|CONNECTICUT|      BLOOD COUGH| PET SCAN|MALIGNANT|THORACIC|    1|  SURGERY|     YES|\n",
      "|100009|RoMert| 54|  M|    FLORIDA|      BLOOD COUGH| PET SCAN|MALIGNANT|THORACIC|    1|  SURGERY|     YES|\n",
      "+------+------+---+---+-----------+-----------------+---------+---------+--------+-----+---------+--------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create a DataFrame\n",
    "cancerdataDF = spark.read.csv(\"/home/ubuntu/data/CancerData.csv\",header=True,inferSchema=True)\n",
    "\n",
    "#Convert DataFrame to Temp Table or Permanent Table\n",
    "cancerdataDF.registerTempTable(\"CancerData\")\n",
    "\n",
    "#Apply Native SQL Queries\n",
    "print(\"All rows and all columns:\")\n",
    "print(\"*\"*30)\n",
    "spark.sql(\"select * from CancerData\").show(5)\n",
    "\n",
    "print(\"All rows but few columns:\")\n",
    "print(\"*\"*30)\n",
    "spark.sql(\"select ID,Name,Age,Sex from CancerData\").show(5)\n",
    "\n",
    "print(\"All rows but only gender=male and age>50:\")\n",
    "print(\"*\"*30)\n",
    "spark.sql(\"select * from CancerData where Sex='M' and Age>50\").show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd1f633f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
